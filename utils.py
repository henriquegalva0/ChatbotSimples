from llama_cpp import Llama
import re

caminho = "./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"

chatbot = Llama(
    model_path=caminho,
    n_ctx=2048,
    n_threads=6,
    verbose=False,
)

historico_chat = """<|system|>
Seu perfil:\n Seu nome √© "Flokebot" e voc√™ √© o assistente virtual respons√°vel pelo atendimento do mercado 'Floke'. Voc√™ deve responder de modo carism√°tico, atencioso e sempre em portugu√™s.\n
Sua miss√£o:\n Voc√™ deve responder d√∫vidas de forma humanizada, correta e coerente a respeito de informa√ß√µes do mercado.\n
Sobre o mercado:\n O mercado se chama 'Floke'. O mercado funciona das 9 horas da manh√£ at√© as 21 horas da noite. O mercado foi fundado na data 21/05/2003. No estoque do mercado h√°: 30 frutas, 21 produtos de limpeza, 50 bebidas diferentes e 28 legumes. O mercado n√£o disponibiliza reembolso.\n
Observa√ß√µes:\n N√£o utilize linguagem t√©cnica!\n N√£o responda d√∫vidas que n√£o sejam relacionadas ao mercado!\n Sempre fale bem da infraestrutura e da confian√ßa do mercado!\n Responda somente a pergunta, ou seja, n√£o fale informa√ß√µes que n√£o tenham rela√ß√£o com a pergunta.\n Utilize linguagem compreens√≠vel de acordo com a norma comum da lingua portuguesa.\n
<|end|>\n
"""

while True:
    input_usuario = input(" Voc√™: ")
    if input_usuario.lower() in ["sair", "exit", "quit"]:
        break

    historico_chat += f"<|user|>\n{input_usuario}\n<|end|>\n<|assistant|>\n"

    chamar_resposta = chatbot(historico_chat, max_tokens=512, stop=["<|end|>"])
    escolher_resposta = chamar_resposta['choices'][0]['text'].strip()
    output_chatbot = re.split(r"<\|user\|>", escolher_resposta)[0].strip()

    print(f"""
    üë§ Voc√™: {input_usuario}
    ü§ñ Flokebot: {output_chatbot}
    """)

    historico_chat += output_chatbot + "\n<|end|>\n"
